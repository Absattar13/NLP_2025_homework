{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "k1gpzj4guo8e1riwj3om1k",
        "id": "Y4WKtovymi6n"
      },
      "source": [
        "### N-gram language models or how to write scientific papers (4 pts)\n",
        "\n",
        "We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n",
        "\n",
        "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
        "\n",
        "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n",
        "\n",
        "_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellId": "u8jdaiy68oib3jvr4k01",
        "id": "mCZmB-lRmi6p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellId": "0c76vnyl3zui9yhtkodgrlf",
        "id": "wz0p60N5mi6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "outputId": "e429b3a3-8f6a-4bcf-f925-cafd77a756b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-30 13:30:32--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/0mulrothty5o8i8ud9gz2/arxivData.json.tar.gz?rlkey=n759u5qx2xpxxglmrl390vwvk&dl=1 [following]\n",
            "--2025-10-30 13:30:32--  https://www.dropbox.com/scl/fi/0mulrothty5o8i8ud9gz2/arxivData.json.tar.gz?rlkey=n759u5qx2xpxxglmrl390vwvk&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com/cd/0/inline/C0Pxoyn-58JMcAP_4ziWfgxWNBGSER2znLqtlNW-yyaRVu0IlU_PWvqT0EJCC8UuU7T-Hw83X_AkV7gHDzsuIxroyMsPQy4xBI1-xE-4LDR6606sD3xI6wKfryPhwRda9Rc/file?dl=1# [following]\n",
            "--2025-10-30 13:30:32--  https://ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com/cd/0/inline/C0Pxoyn-58JMcAP_4ziWfgxWNBGSER2znLqtlNW-yyaRVu0IlU_PWvqT0EJCC8UuU7T-Hw83X_AkV7gHDzsuIxroyMsPQy4xBI1-xE-4LDR6606sD3xI6wKfryPhwRda9Rc/file?dl=1\n",
            "Resolving ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com (ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com (ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C0PjFhqNhaopVgM_ngmKuGiQlaknAO4pXxnH1qBxOKDelnfIdUyw0yQgOteQH1zPmspvEvDnzSCcykR_1XwLKZGEYq4IjC1HD_xzZHzDZ4NlIt_NSA44XZFiMcsdcy5H0u0mi27ccTT7hxFJ_KHR-WQuYl9khbfC_tbp0KK-GjZRtC27HzVCLRbkWtZgJRgHdEjyF8tn7FEcmDPFXIboK0Qh_9t4MPPv6ULqXoS7atgPWX9oteCsFQIpxwCQk1L05AysOZRal3rrDqveLhlwgxJ5k6uf4d_KW9vpI-1-vK4ThuQPkIgBWTbRxij7nmAA7kpiwvyErTDznFJEvS48PvDwG-_WB4bWyp1rj-3damn58A/file?dl=1 [following]\n",
            "--2025-10-30 13:30:34--  https://ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com/cd/0/inline2/C0PjFhqNhaopVgM_ngmKuGiQlaknAO4pXxnH1qBxOKDelnfIdUyw0yQgOteQH1zPmspvEvDnzSCcykR_1XwLKZGEYq4IjC1HD_xzZHzDZ4NlIt_NSA44XZFiMcsdcy5H0u0mi27ccTT7hxFJ_KHR-WQuYl9khbfC_tbp0KK-GjZRtC27HzVCLRbkWtZgJRgHdEjyF8tn7FEcmDPFXIboK0Qh_9t4MPPv6ULqXoS7atgPWX9oteCsFQIpxwCQk1L05AysOZRal3rrDqveLhlwgxJ5k6uf4d_KW9vpI-1-vK4ThuQPkIgBWTbRxij7nmAA7kpiwvyErTDznFJEvS48PvDwG-_WB4bWyp1rj-3damn58A/file?dl=1\n",
            "Reusing existing connection to ucc8c2df9517d56982d0e34dd806.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‘arxivData.json.tar.gz’\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  11.4MB/s    in 1.6s    \n",
            "\n",
            "2025-10-30 13:30:36 (11.4 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "7675                       [{'name': 'Arjun Karuvally'}]   21  1802.07401v1   \n",
              "1180   [{'name': 'Arjun Jain'}, {'name': 'Jonathan To...   28   1409.7963v1   \n",
              "20436  [{'name': 'Chen Fu'}, {'name': 'Nevin L. Zhang...   26  1601.06923v2   \n",
              "36238  [{'name': 'S. Mahmoud Zadeh'}, {'name': 'D. M....    9  1604.02523v4   \n",
              "22674                        [{'name': 'Fionn Murtagh'}]    6  1704.01871v1   \n",
              "\n",
              "                                                    link  month  \\\n",
              "7675   [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "1180   [{'rel': 'alternate', 'href': 'http://arxiv.or...      9   \n",
              "20436  [{'rel': 'alternate', 'href': 'http://arxiv.or...      1   \n",
              "36238  [{'rel': 'alternate', 'href': 'http://arxiv.or...      4   \n",
              "22674  [{'rel': 'alternate', 'href': 'http://arxiv.or...      4   \n",
              "\n",
              "                                                 summary  \\\n",
              "7675   One popular generative model that has high-qua...   \n",
              "1180   In this work, we propose a novel and efficient...   \n",
              "20436  Objective: To treat patients with vascular mil...   \n",
              "36238  The AUV three-dimension path planning in compl...   \n",
              "22674  Cluster analysis of very high dimensional data...   \n",
              "\n",
              "                                                     tag  \\\n",
              "7675   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "1180   [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "20436  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
              "36238  [{'term': 'cs.RO', 'scheme': 'http://arxiv.org...   \n",
              "22674  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "\n",
              "                                                   title  year  \n",
              "7675   A Study into the similarity in generator and d...  2018  \n",
              "1180   MoDeep: A Deep Learning Framework Using Motion...  2014  \n",
              "20436  Identification and classification of TCM syndr...  2016  \n",
              "36238  Differential Evolution for Efficient AUV Path ...  2016  \n",
              "22674  Massive Data Clustering in Moderate Dimensions...  2017  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7502c8b2-e149-45ac-a02e-0195aa98b28d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7675</th>\n",
              "      <td>[{'name': 'Arjun Karuvally'}]</td>\n",
              "      <td>21</td>\n",
              "      <td>1802.07401v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>One popular generative model that has high-qua...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Study into the similarity in generator and d...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1180</th>\n",
              "      <td>[{'name': 'Arjun Jain'}, {'name': 'Jonathan To...</td>\n",
              "      <td>28</td>\n",
              "      <td>1409.7963v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>9</td>\n",
              "      <td>In this work, we propose a novel and efficient...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>MoDeep: A Deep Learning Framework Using Motion...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20436</th>\n",
              "      <td>[{'name': 'Chen Fu'}, {'name': 'Nevin L. Zhang...</td>\n",
              "      <td>26</td>\n",
              "      <td>1601.06923v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>1</td>\n",
              "      <td>Objective: To treat patients with vascular mil...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Identification and classification of TCM syndr...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36238</th>\n",
              "      <td>[{'name': 'S. Mahmoud Zadeh'}, {'name': 'D. M....</td>\n",
              "      <td>9</td>\n",
              "      <td>1604.02523v4</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>4</td>\n",
              "      <td>The AUV three-dimension path planning in compl...</td>\n",
              "      <td>[{'term': 'cs.RO', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Differential Evolution for Efficient AUV Path ...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22674</th>\n",
              "      <td>[{'name': 'Fionn Murtagh'}]</td>\n",
              "      <td>6</td>\n",
              "      <td>1704.01871v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>4</td>\n",
              "      <td>Cluster analysis of very high dimensional data...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Massive Data Clustering in Moderate Dimensions...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7502c8b2-e149-45ac-a02e-0195aa98b28d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7502c8b2-e149-45ac-a02e-0195aa98b28d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7502c8b2-e149-45ac-a02e-0195aa98b28d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7b6115de-c8fb-4b23-a60d-0c34fe5bf2cd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b6115de-c8fb-4b23-a60d-0c34fe5bf2cd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7b6115de-c8fb-4b23-a60d-0c34fe5bf2cd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'name': 'Arjun Jain'}, {'name': 'Jonathan Tompson'}, {'name': 'Yann LeCun'}, {'name': 'Christoph Bregler'}]\",\n          \"[{'name': 'Fionn Murtagh'}]\",\n          \"[{'name': 'Chen Fu'}, {'name': 'Nevin L. Zhang'}, {'name': 'Bao Xin Chen'}, {'name': 'Zhou Rong Chen'}, {'name': 'Xiang Lan Jin'}, {'name': 'Rong Juan Guo'}, {'name': 'Zhi Gang Chen'}, {'name': 'Yun Ling Zhang'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 6,\n        \"max\": 28,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          28,\n          6,\n          26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1409.7963v1\",\n          \"1704.01871v1\",\n          \"1601.06923v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1409.7963v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1409.7963v1', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1704.01871v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1704.01871v1', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1601.06923v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1601.06923v2', 'type': 'application/pdf', 'title': 'pdf'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          9,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"In this work, we propose a novel and efficient method for articulated human\\npose estimation in videos using a convolutional network architecture, which\\nincorporates both color and motion features. We propose a new human body pose\\ndataset, FLIC-motion, that extends the FLIC dataset with additional motion\\nfeatures. We apply our architecture to this dataset and report significantly\\nbetter performance than current state-of-the-art pose detection systems.\",\n          \"Cluster analysis of very high dimensional data can benefit from the\\nproperties of such high dimensionality. Informally expressed, in this work, our\\nfocus is on the analogous situation when the dimensionality is moderate to\\nsmall, relative to a massively sized set of observations. Mathematically\\nexpressed, these are the dual spaces of observations and attributes. The point\\ncloud of observations is in attribute space, and the point cloud of attributes\\nis in observation space. In this paper, we begin by summarizing various\\nperspectives related to methodologies that are used in multivariate analytics.\\nWe draw on these to establish an efficient clustering processing pipeline, both\\npartitioning and hierarchical clustering.\",\n          \"Objective: To treat patients with vascular mild cognitive impairment (VMCI)\\nusing TCM, it is necessary to classify the patients into TCM syndrome types and\\nto apply different treatments to different types. We investigate how to\\nproperly carry out the classification using a novel data-driven method known as\\nlatent tree analysis.\\n  Method: A cross-sectional survey on VMCI was carried out in several regions\\nin northern China from 2008 to 2011, which resulted in a data set that involves\\n803 patients and 93 symptoms. Latent tree analysis was performed on the data to\\nreveal symptom co-occurrence patterns, and the patients were partitioned into\\nclusters in multiple ways based on the patterns. The patient clusters were\\nmatched up with syndrome types, and population statistics of the clusters are\\nused to quantify the syndrome types and to establish classification rules.\\n  Results: Eight syndrome types are identified: Qi Deficiency, Qi Stagnation,\\nBlood Deficiency, Blood Stasis, Phlegm-Dampness, Fire-Heat, Yang Deficiency,\\nand Yin Deficiency. The prevalence and symptom occurrence characteristics of\\neach syndrome type are determined. Quantitative classification rules are\\nestablished for determining whether a patient belongs to each of the syndrome\\ntypes.\\n  Conclusions: A solution for the TCM syndrome classification problem\\nassociated with VMCI is established based on the latent tree analysis of\\nunlabeled symptom survey data. The results can be used as a reference in clinic\\npractice to improve the quality of syndrome differentiation and to reduce\\ndiagnosis variances across physicians. They can also be used for patient\\nselection in research projects aimed at finding biomarkers for the syndrome\\ntypes and in randomized control trials aimed at determining the efficacy of TCM\\ntreatments of VMCI.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '62H30, 91C20', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'H.3.3; I.5.3', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"MoDeep: A Deep Learning Framework Using Motion Features for Human Pose\\n  Estimation\",\n          \"Massive Data Clustering in Moderate Dimensions from the Dual Spaces of\\n  Observation and Attribute Data Clouds\",\n          \"Identification and classification of TCM syndrome types among patients\\n  with vascular mild cognitive impairment using latent tree analysis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2014,\n        \"max\": 2018,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2014,\n          2017,\n          2018\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellId": "lbyqb5rx7j8jpo591r06ak",
        "id": "Btkt7tYAmi6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596c30ae-5518-40a9-ee87-af2c39be627a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# assemble lines: concatenate title and description\n",
        "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "7u97m5s8ekl5zd5a43a1yc",
        "id": "dy15AQsMmi6q"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellId": "u8rvfk719iek97t3rarwr",
        "id": "XhdSWUIGmi6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a817f09-9947-4698-e297-96818a34b0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['differential contrastive divergence ; this paper has been retracted .', 'what does artificial life tell us about death ? ; short philosophical essay', 'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .']\n"
          ]
        }
      ],
      "source": [
        "# Task: convert lines (in-place) into strings of space-separated tokens. Import & use WordPunctTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "lines = [' '.join(tokenizer.tokenize(line.lower())) for line in lines]\n",
        "print(sorted(lines, key=len)[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellId": "w88nddpp2k8edoeyyyjh0l",
        "id": "yzyeOVU_mi6q"
      },
      "outputs": [],
      "source": [
        "assert sorted(lines, key=len)[0] == \\\n",
        "    'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == \\\n",
        "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "qb6h3hxmr095egzv8rlzul",
        "id": "0rL1aNOCmi6q"
      },
      "source": [
        "### N-Gram Language Model (1point)\n",
        "\n",
        "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
        "\n",
        "It can do so by following the chain rule:\n",
        "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$\n",
        "\n",
        "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
        "\n",
        "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
        "\n",
        "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
        "\n",
        "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words.\n",
        "\n",
        "$$\n",
        "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
        "$$\n",
        "\n",
        "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "u68wydbiioqlp5gl96mhd",
        "id": "I8eHF0Wsmi6q"
      },
      "source": [
        "The first stage to building such a model is counting all word occurences given N-1 previous words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellId": "og84gjipnumsakhiiu9ap",
        "id": "MzFUxodJmi6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73668a4-5601-4933-9aec-2800384fa666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 15301.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "def count_ngrams(lines, n):\n",
        "    counts = defaultdict(Counter)\n",
        "\n",
        "    for line in tqdm(lines):\n",
        "        tokens = line.split()\n",
        "        tokens.append(EOS)\n",
        "        for i in range(len(tokens)):\n",
        "            start = i - (n - 1)\n",
        "            prefix = tokens[start:i] if start >= 0 else [UNK] * (-start) + tokens[0:i]\n",
        "            prefix = tuple(prefix[-(n - 1):]) if n > 1 else tuple()\n",
        "            next_token = tokens[i]\n",
        "            counts[prefix][next_token] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts[('_UNK_', 'a')]['note'] == 3\n",
        "assert dummy_counts[('p', '=')]['np'] == 2\n",
        "assert dummy_counts[('author', '.')][EOS] == 1\n",
        "print(\"All tests passed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellId": "xyf2he6lak9mmqarl3nck",
        "id": "kR6ayRyVmi6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45dbc93b-d6ba-4473-8d4f-522dda7d1388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 17383.55it/s]\n"
          ]
        }
      ],
      "source": [
        "# let's test it\n",
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "4j620npeqvj0k8ak8xqx8xk",
        "id": "UqW_tI2Nmi6r"
      },
      "source": [
        "Once we can count N-grams, we can build a probabilistic language model.\n",
        "The simplest way to compute probabilities is in proporiton to counts:\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellId": "c7cm76wmzlaa12bctznzei",
        "id": "i2pD4V9xmi6r"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "class NGramLanguageModel:\n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\"\n",
        "        Train a simple count-based language model:\n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.probs = defaultdict(Counter)\n",
        "        for prefix, next_tokens in counts.items():\n",
        "            total = sum(next_tokens.values())\n",
        "            for token, cnt in next_tokens.items():\n",
        "                self.probs[prefix][token] = cnt / total\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : probability}\n",
        "        \"\"\"\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [UNK] * (self.n - 1 - len(prefix)) + prefix\n",
        "        return self.probs[tuple(prefix)]\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\"\n",
        "        Returns P(next_token | prefix)\n",
        "        \"\"\"\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "0ftnn4nmuzrup6c0vvhb8q",
        "id": "pRUDgfA8mi6r"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellId": "a7zajcnvhqupvcrmacvkur",
        "id": "Vl-kvzPmmi6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b791aa-eb27-4cd3-8375-11c0d7288312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 10682.04it/s]\n"
          ]
        }
      ],
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "oh8r9a41kuk4r51wra9",
        "id": "TdQu6ZqVmi6r"
      },
      "source": [
        "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellId": "f17xoejjppmooo2nopw4xo",
        "id": "fzD0eXjBmi6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b7b22f-4d2a-469a-b420-ff921596dc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41000/41000 [00:18<00:00, 2267.51it/s]\n"
          ]
        }
      ],
      "source": [
        "lm = NGramLanguageModel(lines, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "2kd9glwnkr470qc4bt7f1e",
        "id": "pY-_4aJ-mi6r"
      },
      "source": [
        "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
        "\n",
        "$ X = [] $\n",
        "\n",
        "__forever:__\n",
        "* $w_{next} \\sim P(w_{next} | X)$\n",
        "* $X = concat(X, w_{next})$\n",
        "\n",
        "\n",
        "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n",
        "\n",
        "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
        "\n",
        "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellId": "sgbatlm9vzb4z889fho7",
        "id": "9Bc8GF8Kmi6r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "    \"\"\"\n",
        "    return next token after prefix;\n",
        "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    probs_dict = lm.get_possible_next_tokens(prefix)\n",
        "    if not probs_dict:\n",
        "        return EOS\n",
        "\n",
        "    tokens = np.array(list(probs_dict.keys()))\n",
        "    probs = np.array(list(probs_dict.values()), dtype=np.float64)\n",
        "    if temperature == 0:\n",
        "        return tokens[np.argmax(probs)]\n",
        "    adjusted_probs = probs ** (1.0 / temperature)\n",
        "    adjusted_probs = adjusted_probs / adjusted_probs.sum()\n",
        "    next_token = np.random.choice(tokens, p=adjusted_probs)\n",
        "    return next_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellId": "98l40131wjtd5xbdm5b2nr",
        "id": "xoXoHuSlmi6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b831852-1b91-4fd4-efe0-f162b032756b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks nice!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000\n",
        "\n",
        "print(\"Looks nice!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "ux4n8iq523n4s3ftrelhxj",
        "id": "1uS9qMQ5mi6r"
      },
      "source": [
        "Let's have fun with this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellId": "1nnnycga61rijt6nd8zai",
        "id": "GiGqrpnxmi6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691a0e35-60af-4887-ddad-31971b2e5731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial learning in dialogue , request dialogue , accurate , while other distributed multi - task learning to compose the images . an urgent requirement to determine the correct viewpoint of wqos to the signal . a . k - medoids partition the vertices in dendrograms are given a network of cameras with stereo images , in image and the ability of the mammalian brain appears alongside other layers . these networks is very effective way of their objects . they produce a hidden markov models . this includes the two sorted clp language , profanity or abusive statements . conventional\n"
          ]
        }
      ],
      "source": [
        "prefix = 'artificial' # <- your ideas :)\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellId": "pxyjsv3b7r8thdfxlgitl",
        "id": "YNxO8AEkmi6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13b194f-9aca-45a8-86e1-1972bbc40dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bridging the gap between the two - dimensional data . the results of the network . we propose a new approach to the best of our approach is based on the complexity of the new model , which we call this technique , we apply a state - of - the - art . _EOS_\n"
          ]
        }
      ],
      "source": [
        "prefix = 'bridging the' # <- more of your ideas\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "2n90bscmzfko0qnctp7ysc",
        "id": "sEzYsAfOmi6s"
      },
      "source": [
        "__More in the homework:__ nucleus sampling, top-k sampling, beam search(not for the faint of heart)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "3gdmey7g8at5n5c5x4gayh",
        "id": "RhTGYM00mi6s"
      },
      "source": [
        "### Evaluating language models: perplexity (1point)\n",
        "\n",
        "Perplexity is a measure of how well your model approximates the true probability distribution behind the data. __Smaller perplexity = better model__.\n",
        "\n",
        "To compute perplexity on one sentence, use:\n",
        "$$\n",
        "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
        "$$\n",
        "\n",
        "\n",
        "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of $1/N$, where $N$ is __total length (in tokens) of all sentences__ in corpora.\n",
        "\n",
        "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellId": "5hp010xyzzb4vqewo1bhny",
        "id": "Ov5waQKQmi6s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    \"\"\"\n",
        "    :param lines: list of strings (each line = space-separated tokens)\n",
        "    :param min_logprob: lower bound for log-probabilities\n",
        "    :returns: corpus-level perplexity\n",
        "    \"\"\"\n",
        "    total_logprob = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.strip().split() + [EOS]\n",
        "        prefix = []\n",
        "        for token in tokens:\n",
        "            log_p = np.log(lm.get_next_token_prob(prefix, token) + 1e-12)\n",
        "            log_p = max(log_p, min_logprob)\n",
        "            total_logprob += log_p\n",
        "            total_tokens += 1\n",
        "            prefix.append(token)\n",
        "    avg_logprob = total_logprob / total_tokens\n",
        "    ppl = np.exp(-avg_logprob)\n",
        "\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellId": "8b689bobhkey04x7pabupj",
        "id": "Uy-UoL9umi6s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    \"\"\"\n",
        "    :param lines: list of strings (each line = space-separated tokens)\n",
        "    :param min_logprob: lower bound for log-probabilities\n",
        "    :returns: corpus-level perplexity\n",
        "    \"\"\"\n",
        "    total_logprob = 0.0\n",
        "    total_tokens = 0\n",
        "    for line in lines:\n",
        "        tokens = line.strip().split()\n",
        "        prefix = []\n",
        "        for token in tokens + [EOS]:\n",
        "            prob = lm.get_next_token_prob(prefix, token)\n",
        "            logprob = np.log(prob) if prob > 0 else min_logprob\n",
        "            logprob = max(logprob, min_logprob)\n",
        "            total_logprob += logprob\n",
        "            total_tokens += 1\n",
        "            prefix.append(token)\n",
        "    avg_logprob = total_logprob / total_tokens\n",
        "    ppl = np.exp(-avg_logprob)\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "ypc4lks4vs1li908fqi8",
        "id": "rRWe4k6Xmi6s"
      },
      "source": [
        "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellId": "tjnehsem2lmijkg2lto4w",
        "id": "9NBKan1Imi6s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "UNK = \"_UNK_\"\n",
        "EOS = \"_EOS_\"\n",
        "def count_ngrams(lines, n, min_count=1):\n",
        "    \"\"\"\n",
        "    Считает, сколько раз каждый токен встречается после (n-1) предыдущих.\n",
        "    \"\"\"\n",
        "    token_counts = Counter()\n",
        "    for line in lines:\n",
        "        token_counts.update(line.split())\n",
        "    def process_line(line):\n",
        "        tokens = [t if token_counts[t] >= min_count else UNK for t in line.split()]\n",
        "        tokens += [EOS]\n",
        "        return tokens\n",
        "    counts = defaultdict(Counter)\n",
        "    for line in lines:\n",
        "        tokens = process_line(line)\n",
        "        for i in range(len(tokens)):\n",
        "            prefix = tokens[max(0, i - n + 1):i]\n",
        "            prefix = [UNK] * (n - 1 - len(prefix)) + prefix\n",
        "            nex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cellId": "38nfbfkpzgfxik8kccyt1l",
        "id": "i-EQWgbrmi6s"
      },
      "outputs": [],
      "source": [
        "# whoops, it just blew up :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "oopn2o57wxm9vbxzycytce",
        "id": "i1FUgS5Dmi6s"
      },
      "source": [
        "### LM Smoothing\n",
        "\n",
        "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
        "\n",
        "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
        "\n",
        "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
        "\n",
        "Here's an example code we've implemented for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellId": "ioh26rlov6g8l2ssj1c8pm",
        "id": "zaqS4MMEmi6s"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel):\n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "90vsann3920ie05r2blbmi",
        "execution_id": "3868303d-0bb9-42c6-a9a8-dcf485c8220c",
        "id": "SkzS-3y5mi6s"
      },
      "source": [
        "**Disclaimer**: the implementation above assumes all words unknown within a given context to be equally likely, *as well as the words outside of vocabulary*. Therefore, its' perplexity will be lower than it should when encountering such words. Therefore, comparing it with a model with fewer unknown words will not be fair. When implementing your own smoothing, you may handle this by adding a virtual `UNK` token of non-zero probability. Technically, this will result in a model where probabilities do not add up to $1$, but it is close enough for a practice excercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellId": "3xvxkdxcmfqucruyt66mdc",
        "id": "lN2PiHiami6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564fc1ef-79d5-4914-e665-0560b54690c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N=1, total probability sum = 1.000000\n",
            "N=2, total probability sum = 1.000000\n",
            "N=3, total probability sum = 1.000000\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
        "    total_prob = sum([dummy_lm.get_next_token_prob(['a'], w_i) for w_i in dummy_lm.vocab])\n",
        "    print(f\"N={n}, total probability sum = {total_prob:.6f}\")\n",
        "    assert np.allclose(total_prob, 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "cellId": "j6zqa50koitjjri9ipd8ec",
        "id": "Ogt8B6Dcmi6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ec4286-0937-494e-872c-717f0ff97d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1, Perplexity = 53543.26887\n",
            "N = 2, Perplexity = 498.98755\n",
            "N = 3, Perplexity = 3779.14562\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellId": "pjuqt30jcerwbz1ym9zv1",
        "id": "o2h-qbPWmi6s"
      },
      "outputs": [],
      "source": [
        "# optional: try to sample tokens from such a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "3b8s1y9uls4fosu3yp28gg",
        "id": "90lOAt0rmi6s"
      },
      "source": [
        "### Kneser-Ney smoothing (2 points)\n",
        "\n",
        "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
        "\n",
        "\n",
        "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
        "\n",
        "It can be computed recurrently, for n>1:\n",
        "\n",
        "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
        "\n",
        "where\n",
        "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
        "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
        "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
        "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
        "\n",
        "See lecture slides or wiki for more detailed formulae.\n",
        "\n",
        "__Your task__ is to\n",
        "- implement `KneserNeyLanguageModel` class,\n",
        "- test it on 1-3 gram language models\n",
        "- find optimal (within reason) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "cellId": "2ix7kzw02v30oye55322all",
        "id": "lhu078qEmi6s"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "\n",
        "class KneserNeyLanguageModel(NGramLanguageModel):\n",
        "    \"\"\" Kneser-Ney smoothing language model \"\"\"\n",
        "    def __init__(self, lines, n, delta=0.75):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.counts = count_ngrams(lines, n)\n",
        "        self.lower_counts = count_ngrams(lines, n - 1)\n",
        "        self.vocab = set(token for token_counts in self.counts.values() for token in token_counts)\n",
        "        continuation_counts = Counter()\n",
        "        for prefix in self.counts:\n",
        "            for token in self.counts[prefix]:\n",
        "                continuation_counts[token] += 1\n",
        "        self.continuation_counts = continuation_counts\n",
        "        self.total_continuations = sum(continuation_counts.values())\n",
        "    def continuation_prob(self, token):\n",
        "        \"\"\" Unigram Kneser-Ney probability (probability of seeing token in new context) \"\"\"\n",
        "        return self.continuation_counts[token] / self.total_continuations\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\" Recursive Kneser-Ney smoothing \"\"\"\n",
        "        prefix = tuple(prefix[-(self.n - 1):])\n",
        "        if self.n == 1:\n",
        "            return self.continuation_prob(next_token)\n",
        "        count_prefix = sum(self.counts[prefix].values()) if prefix in self.counts else 0\n",
        "        count_ngram = self.counts[prefix][next_token] if next_token in self.counts.get(prefix, {}) else 0\n",
        "        p_cont = 0\n",
        "        if count_prefix > 0:\n",
        "            p_cont = max(count_ngram - self.delta, 0) / count_prefix\n",
        "        unique_continuations = len(self.counts[prefix]) if prefix in self.counts else 0\n",
        "        lambda_prefix = (self.delta * unique_continuations / count_prefix) if count_prefix > 0 else 0\n",
        "        lower_model = KneserNeyLanguageModel([], n=self.n - 1, delta=self.delta)\n",
        "        lower_model.counts = self.lower_counts\n",
        "        lower_model.continuation_counts = self.continuation_counts\n",
        "        lower_model.total_continuations = self.total_continuations\n",
        "        return p_cont + lambda_prefix * lower_model.get_next_token_prob(prefix[1:], next_token)\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\" Возвращает распределение вероятностей всех слов \"\"\"\n",
        "        return {token: self.get_next_token_prob(prefix, token) for token in self.vocab}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellId": "lsk91832qbmdt7x1q0a8z4",
        "id": "FyVhP8Rkmi6s"
      },
      "outputs": [],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "cellId": "pp3jtkk9annp1qkou58x1b",
        "id": "CYpTzEpOmi6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f2df82-003d-4a56-81e2-70b7cb9022f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1, Perplexity = 198615.81919\n",
            "N = 2, Perplexity = 1499.05130\n",
            "N = 3, Perplexity = 417272762.71935\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = KneserNeyLanguageModel(train_lines, n=n, delta=0.75)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
    "notebookPath": "seminar.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}